In order to run the python code, you'll need these installed:
"""
Need these modules installed:
!pip install langchain
!pip install sentence-transformers
!pip install faiss-cpu
!pip install bs4
!pip install replicate
!pip install langchain-community
"""


The python code is largely from this tutorial:
https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/RAG/hello_llama_cloud.ipynb

but translated to use a local Llama model instead of the Replicate one



Talk about ollama setup
and llama download process


also include some demo videos of me using it



How do you prompt in a way that emphasizes collaborative learning. Make it a conversational partner and not an authoritative boss

How do you load the RAG documents faster?

Can I get the model to run on GUI?

Maybe make a GUI where you can:
- see whether it's running
- upload your own articles, or turn on/off articles
- see original article sources
- have the actual conversation, and keep track of how long it's been thinking
- maybe can have the reflection available too
Stuff I've Noticed While Working Here:

- webscraping:
    "There are reasonable limits to concurrent requests, defaulting to 2 per second. If you aren't concerned about being a good citizen, or you control the server you are scraping and don't care about load, you can change the requests_per_second parameter to increase the max concurrent requests. Note, while this will speed up the scraping process, but may cause the server to block you. Be careful!"
    from https://python.langchain.com/docs/integrations/document_loaders/web_base/


Llama's documentation also had interesting things:
- https://huggingface.co/meta-llama/Llama-3.1-8B
- carbon impact
- intended uses
- considerations (ethics)


Think about dataset ethics:
- is it okay to ignore the copyright for this small project? we do it most of the time in academia as students anyway
- maybe there's a way to cite the sources


Tried to do stuff with cloud compute, but ran into my monthly spending limit in like an hour.

# need to figure out how much parsing it through these documents actually affects the results

# need to figure out if the webloader can load multiple sites at once:
    # might help with dataset ethics
    # bcuz otherwise my page is functionally a plagiarism page